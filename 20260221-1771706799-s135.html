<!DOCTYPE html>
<html lang="zh-TW">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Task Output</title>
    <style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, sans-serif;
            line-height: 1.6;
            max-width: 900px;
            margin: 0 auto;
            padding: 20px;
            background: #f5f5f5;
        }
        .container {
            background: white;
            padding: 40px;
            border-radius: 8px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }
        h1 {
            color: #333;
            border-bottom: 3px solid #007bff;
            padding-bottom: 10px;
        }
        h2 {
            color: #555;
            margin-top: 30px;
        }
        code {
            background: #f4f4f4;
            padding: 2px 6px;
            border-radius: 3px;
        }
        pre {
            background: #f4f4f4;
            padding: 15px;
            border-radius: 5px;
            overflow-x: auto;
        }
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
        }
        th, td {
            border: 1px solid #ddd;
            padding: 12px;
            text-align: left;
        }
        th {
            background: #007bff;
            color: white;
        }
        .footer {
            text-align: center;
            margin-top: 40px;
            color: #666;
            font-size: 0.9em;
        }
        .back-link {
            display: inline-block;
            margin-top: 20px;
            color: #007bff;
            text-decoration: none;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>Task Output</h1>
<p><strong>Task ID:</strong> 20260221-1771706799-s135
<strong>Agent:</strong> Charlie Research
<strong>Status:</strong> completed
<strong>Timestamp:</strong> 2026-02-22T05:34:00Z</p>
<h2>Research Summary</h2>
<p>This research investigates error propagation and model collapse in diffusion models, focusing on theoretical frameworks that explain how recursively training models on synthetic data affects performance. The study establishes mathematical bounds on divergence accumulation and identifies different regimes of model behavior based on score estimation error and fresh data proportions.</p>
<h2>Key Findings</h2>
<ol>
<li>
<p><strong>Intra-generation Divergence Bounds</strong> ‚Äî The research establishes that intra-generational divergence œá¬≤(pÃÇ‚Å±‚Å∫¬π‚à•q·µ¢) is equivalent to the pathwise score-error energy Œµ‚ãÜ,·µ¢¬≤ under observability conditions, providing the first lower bounds for diffusion models quantifying learned vs. target distribution discrepancies. | Source: arXiv:2602.16601</p>
</li>
<li>
<p><strong>Error Accumulation Dichotomy</strong> ‚Äî Two distinct regimes emerge: if ‚àëŒµ‚ãÜ,‚Çñ¬≤ = ‚àû, accumulated divergence cannot vanish; if ‚àëŒµ‚ãÜ,‚Çñ¬≤ &lt; ‚àû, divergence remains stable and bounded, following a geometrically-discounted accumulation law. | Source: arXiv:2602.16601</p>
</li>
<li>
<p><strong>Data Accumulation Prevents Collapse</strong> ‚Äî Unlike data replacement scenarios which lead to model collapse, accumulating synthetic data alongside original real data results in finite upper bounds on test error, preventing collapse across model architectures and data types. | Source: arXiv:2404.01413</p>
</li>
<li>
<p><strong>Recursive Stability Framework</strong> ‚Äî The introduction of recursive stability explains how both model architecture and the proportion between real and synthetic data determine the success of self-consuming training loops. | Source: arXiv:2502.18865</p>
</li>
<li>
<p><strong>Modular Error Propagation Theory</strong> ‚Äî Diffusion models suffer from error propagation through three interconnected elements: modular error, cumulative error, and propagation equation, with cumulative error directly impacting generation quality. | Source: arXiv:2308.05021</p>
</li>
</ol>
<h2>Detailed Analysis</h2>
<h3>Theoretical Framework for Error Propagation</h3>
<p>The main paper (arXiv:2602.16601) establishes a comprehensive theoretical framework for understanding error propagation in score-based diffusion models under recursive training. The framework analyzes a realistic pipeline where each training round uses a combination of synthetic data and fresh samples from the target distribution, obtaining upper and lower bounds on accumulated divergence between generated and target distributions.</p>
<h4>Key Mathematical Components</h4>
<p>The research tracks two primary quantities:
- <strong>Accumulated divergence</strong>: D·µ¢ = œá¬≤(pÃÇ‚Å±‚à•p_data) measuring divergence between the i-th generation model and true data distribution
- <strong>Intra-generation divergence</strong>: I·µ¢ = œá¬≤(pÃÇ‚Å±‚Å∫¬π‚à•q·µ¢) measuring error introduced in one training round</p>
<p>The training mixture is defined as:
q·µ¢ = Œ±¬∑p_data + (1-Œ±)¬∑pÃÇ‚Å±</p>
<p>where Œ± represents the proportion of fresh samples and (1-Œ±) represents synthetic samples.</p>
<h3>Fundamental Theoretical Results</h3>
<h4>Intra-generation Divergence Control</h4>
<p>The research establishes a crucial two-sided bound for intra-generation divergence:</p>
<p><strong>Theorem 3.4</strong>: Under appropriate assumptions, for sufficiently small score errors:
œá¬≤(pÃÇ‚Å±‚Å∫¬π‚à•q·µ¢) ‚âç Œµ‚ãÜ,·µ¢¬≤</p>
<p>This shows that intra-generational divergence is controlled by pathwise score-error energy and they are equivalent up to constants, accounting for observability and tail effects.</p>
<h4>Error Observability Coefficient</h4>
<p>A key innovation is the <strong>observability coefficient</strong> Œ∑·µ¢, which quantifies how visible score errors are at the endpoint:</p>
<p>Œ∑·µ¢ = Var‚Ñô·µ¢‚ãÜ(ùîº[M_T·µ¢‚à£Y_{t‚ÇÄ}‚Å±,‚ãÜ]) / Œµ‚ãÜ,·µ¢¬≤</p>
<p>When Œ∑·µ¢ &gt; 0, score errors leave detectable imprints on the terminal distribution, which is typically the case for state-dependent perturbations in practical neural networks.</p>
<h4>Long-term Error Accumulation</h4>
<p>The research identifies two distinct regimes for error accumulation:</p>
<p><strong>Proposition 4.1</strong>: 
1. If ‚àë·µ¢‚â•‚ÇÄ Œµ‚ãÜ,·µ¢¬≤ = ‚àû, then ‚àë·µ¢‚â•‚ÇÄ D·µ¢ = ‚àû and (D·µ¢) cannot converge to 0
2. If Œµ‚ãÜ,·µ¢¬≤ ‚â• ŒµÃÑ &gt; 0 for large i, then lim sup·µ¢‚Üí‚àû D·µ¢ ‚â• Œ±¬∑Œ∑ÃÑ¬∑ŒµÃÑ/(16(1+(1-Œ±)¬≤))</p>
<p>For controlled errors where ‚àëŒµ‚ãÜ,‚Çñ¬≤ &lt; ‚àû, the accumulated divergence follows a <strong>geometrically-discounted decomposition</strong>:</p>
<p>œá¬≤(pÃÇ·¥∫‚Å∫¬π‚à•p_data) + C_bias ‚âç ‚àë‚Çñ=‚ÇÄ·¥∫ (1-Œ±)¬≤‚ÅΩ·¥∫‚Åª·µè‚Åæ ¬∑ Œµ‚ãÜ,‚Çñ¬≤</p>
<p>This shows that errors from m generations in the past are suppressed by a factor (1-Œ±)¬≤·µê, quantifying the effect of fresh data proportion Œ±.</p>
<h3>Related Theoretical Developments</h3>
<h4>Modular Error Framework (arXiv:2308.05021)</h4>
<p>Foundational work establishes that diffusion models suffer from error propagation through three interconnected elements:
1. <strong>Modular error</strong>: errors within individual components
2. <strong>Cumulative error</strong>: accumulated errors across the model
3. <strong>Propagation equation</strong>: relates modular and cumulative errors</p>
<p>This framework provides theoretical confirmation that DMs are indeed affected by error propagation, unlike some sequential models like Conditional Random Fields.</p>
<h4>Recursive Stability (arXiv:2502.18865)</h4>
<p>The concept of <strong>recursive stability</strong> provides the first theoretical generalization analysis explaining discrepancies in STL outcomes. Key findings include:
- Model architecture significantly influences STL success
- Even constant-sized proportions of real data ensure convergence
- The framework extends to transformers in in-context learning
- Provides insights into optimal synthetic data sizing</p>
<h4>Data Accumulation vs. Replacement (arXiv:2404.01413)</h4>
<p>This research demonstrates a crucial distinction:
- <strong>Data replacement</strong>: Leads to model collapse as test error increases with iterations
- <strong>Data accumulation</strong>: Results in finite upper bounds on test error, preventing collapse</p>
<p>Using an analytically tractable framework with linear models, the authors prove that when data accumulate, the test error has a finite upper bound independent of iteration count, meaning model collapse no longer occurs.</p>
<h3>Practical Implications</h3>
<h4>Fresh Data Proportion Optimization</h4>
<p>The theoretical results provide guidance for optimizing the fresh data proportion Œ±:
- <strong>Low Œ± (Œ± ‚âà 0.1)</strong>: Errors persist across many generations, creating wide bands of contributions
- <strong>Medium Œ± (Œ± ‚âà 0.5)</strong>: Intermediate regime with moderate memory decay<br />
- <strong>High Œ± (Œ± ‚âà 0.9)</strong>: Short memory structure where only recent generations dominate</p>
<h4>Model Architecture Considerations</h4>
<p>The research suggests that model architecture plays a crucial role in determining recursive stability. architectures with better error observability (Œ∑·µ¢ &gt; 0) and lower score error energies Œµ‚ãÜ,·µ¢¬≤ will be more stable under recursive training.</p>
<h4>Mitigation Strategies</h4>
<p>Several strategies emerge from the theoretical analysis:
1. <strong>Data accumulation</strong>: Rather than replacing, accumulate synthetic data alongside real data
2. <strong>Fresh data injection</strong>: Maintain sufficient Œ± &gt; 0 to ensure geometric error decay
3. <strong>Score error minimization</strong>: Reduce Œµ‚ãÜ,·µ¢¬≤ through improved score estimation techniques
4. <strong>Architecture design</strong>: Choose architectures with favorable observability properties</p>
<h2>Sources</h2>
<ul>
<li><a href="https://arxiv.org/abs/2602.16601">Error Propagation and Model Collapse in Diffusion Models: A Theoretical Study</a> ‚Äî Main theoretical framework establishing divergence bounds and error accumulation laws in score-based diffusion models</li>
<li><a href="https://arxiv.org/abs/2308.05021">On Error Propagation of Diffusion Models</a> ‚Äî Foundational work introducing modular error, cumulative error, and propagation equation framework</li>
<li><a href="https://arxiv.org/abs/2502.18865">A Theoretical Perspective: How to Prevent Model Collapse in Self-consuming Training Loops</a> ‚Äî Introduces recursive stability and analyzes architecture/data proportion effects on STL success</li>
<li><a href="https://arxiv.org/abs/2404.01413">Is Model Collapse Inevitable? Breaking the Curse of Recursion by Accumulating Real and Synthetic Data</a> ‚Äî Demonstrates that data accumulation prevents model collapse vs. data replacement scenarios</li>
</ul>
<h2>Metadata</h2>
<ul>
<li><strong>Confidence:</strong> high</li>
<li><strong>Research depth:</strong> deep</li>
<li><strong>Data freshness:</strong> February 2026 (most recent source)</li>
<li><strong>Suggestions:</strong> This theoretical work provides foundations for practical mitigation strategies. Further research could explore adaptive Œ± scheduling, architecture-specific observability optimization, and extension to other generative model families like flow-based models</li>
<li><strong>Errors:</strong> None encountered - all relevant sources were successfully accessed and analyzed</li>
</ul>
        <p class="footer">Generated: 2026-02-22 11:29:11</p>
        <a href="index.html" class="back-link">‚Üê Back to Index</a>
    </div>
</body>
</html>
