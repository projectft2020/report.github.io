<!DOCTYPE html>
<html lang="zh-TW">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>åŸºæ–¼ Transformer çš„æ³¢å‹•ç‡é æ¸¬ç ”ç©¶</title>
    <style>
        :root {
            --primary-color: #2563eb;
            --secondary-color: #64748b;
            --bg-color: #f8fafc;
            --text-color: #1e293b;
        }
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            background-color: var(--bg-color);
            max-width: 800px;
            margin: 0 auto;
            padding: 2rem;
        }
        h1 { color: var(--primary-color); margin-bottom: 0.5rem; }
        h2 { color: var(--primary-color); margin-top: 2rem; border-bottom: 2px solid var(--secondary-color); padding-bottom: 0.5rem; }
        h3 { color: var(--secondary-color); margin-top: 1.5rem; }
        .meta { color: var(--secondary-color); font-size: 0.9rem; margin-bottom: 2rem; }
        .updated { background: #fef3c7; padding: 0.5rem 1rem; border-radius: 8px; display: inline-block; margin-bottom: 1rem; font-size: 0.85rem; }
        pre { background: #1e293b; color: #f8fafc; padding: 1rem; border-radius: 8px; overflow-x: auto; }
        code { background: #e2e8f0; padding: 0.2rem 0.4rem; border-radius: 4px; }
        pre code { background: none; padding: 0; }
        table { width: 100%; border-collapse: collapse; margin: 1rem 0; }
        th, td { border: 1px solid #e2e8f0; padding: 0.75rem; text-align: left; }
        th { background: var(--primary-color); color: white; }
        .back-link { display: inline-block; margin-bottom: 2rem; color: var(--primary-color); text-decoration: none; }
        .back-link:hover { text-decoration: underline; }
    </style>
</head>
<body>
    <a href="index.html" class="back-link">â† è¿”å›ç ”ç©¶å ±å‘Šåˆ—è¡¨</a>
    <div class="updated">ğŸ“… æ›´æ–°æ™‚é–“ï¼š2026-02-21 04:04:16</div>
    <div class="content">
<h1>åŸºæ–¼Transformeræ¶æ§‹å’Œæ³¨æ„åŠ›æ©Ÿåˆ¶çš„æ³¢å‹•ç‡é æ¸¬ç ”ç©¶</h1>

<strong>ç ”ç©¶ç·¨è™Ÿï¼š</strong> v001
<strong>ç ”ç©¶æ©Ÿæ§‹ï¼š</strong> Charlie Research
<strong>ç‹€æ…‹ï¼š</strong> å®Œæˆ
<strong>æ™‚é–“æˆ³è¨˜ï¼š</strong> 2026-02-20T17:13:00Z

<h2>ç ”ç©¶æ‘˜è¦</h2>

<p>æœ¬ç ”ç©¶æ·±å…¥æ¢è¨äº†Transformeræ¶æ§‹å’Œæ³¨æ„åŠ›æ©Ÿåˆ¶åœ¨é‡‘èæ³¢å‹•ç‡é æ¸¬ä¸­çš„æ‡‰ç”¨ã€‚é€šéåˆ†æTransformeråœ¨æ™‚é–“åºåˆ—é æ¸¬ä¸­çš„å„ªå‹¢ã€ç ”ç©¶æ³¨æ„åŠ›æ©Ÿåˆ¶å¦‚ä½•æ”¹å–„æ³¢å‹•ç‡é æ¸¬ã€å°æ¯”å‚³çµ±æ–¹æ³•ï¼ˆGARCHã€EWMAç­‰ï¼‰ï¼Œä¸¦æä¾›å¯¦æ–½å»ºè­°å’ŒPythonä»£ç¢¼æ¡†æ¶ï¼Œæœ¬ç ”ç©¶è­‰å¯¦äº†TransformeråŸºç¤æ¨¡å‹åœ¨æ³¢å‹•ç‡é æ¸¬æ–¹é¢é¡¯è‘—å„ªæ–¼å‚³çµ±è¨ˆé‡ç¶“æ¿Ÿå­¸æ–¹æ³•ã€‚</p>

<h2>ä¸»è¦ç™¼ç¾</h2>

<p>1. <strong>Transformeræ¶æ§‹å„ªå‹¢</strong> â€” èƒ½å¤ æ•æ‰é•·æœŸä¾è³´æ€§å’Œéç·šæ€§å‹•æ…‹ï¼Œé©æ‡‰å¸‚å ´çµæ§‹è®ŠåŒ– | Source: MDPIç ”ç©¶
2. <strong>æ³¨æ„åŠ›æ©Ÿåˆ¶æ”¹é€²</strong> â€” é€šéè‡ªæ³¨æ„åŠ›æ©Ÿåˆ¶è­˜åˆ¥æ­·å²æ³¢å‹•ç‡ä¸­çš„é‡è¦æ¨¡å¼ï¼Œæé«˜é æ¸¬æº–ç¢ºæ€§ | Source: Springerè«–æ–‡
3. <strong>æ€§èƒ½å°æ¯”å„ªå‹¢</strong> â€” Transformeræ¨¡å‹åœ¨çŸ­æœŸå’Œé•·æœŸé æ¸¬ä¸­å‡é¡¯è‘—å„ªæ–¼å‚³çµ±GARCHæ¨¡å‹ | Source: å¯¦è­‰åˆ†æ
4. <strong>å¯¦ç¾æ¡†æ¶å¯è¡Œæ€§</strong> â€” åŸºæ–¼PyTorchçš„å¯¦ç¾æä¾›äº†é«˜æ•ˆä¸”å¯æ“´å±•çš„è§£æ±ºæ–¹æ¡ˆ | Source: GitHubä»£ç¢¼åº«</p>

<h2>è©³ç´°åˆ†æ</h2>

<h3>1. Transformeråœ¨æ™‚é–“åºåˆ—é æ¸¬ä¸­çš„å„ªå‹¢</h3>

<p>Transformeræ¶æ§‹ç›¸è¼ƒæ–¼å‚³çµ±æ™‚é–“åºåˆ—æ¨¡å‹å…·æœ‰ä»¥ä¸‹é¡¯è‘—å„ªå‹¢ï¼š</p>

<h4>1.1 é•·æœŸä¾è³´æ€§å»ºæ¨¡</h4>
å‚³çµ±çš„RNNå’ŒLSTMæ¨¡å‹åœ¨è™•ç†é•·åºåˆ—æ™‚æœƒé¢è‡¨æ¢¯åº¦æ¶ˆå¤±å•é¡Œï¼Œè€ŒTransformeré€šéè‡ªæ³¨æ„åŠ›æ©Ÿåˆ¶èƒ½å¤ ç›´æ¥å»ºæ¨¡åºåˆ—ä¸­ä»»æ„å…©å€‹ä½ç½®ä¹‹é–“çš„é—œä¿‚ï¼š

<strong>æ•¸å­¸å…¬å¼ï¼š</strong>
<pre><code>Attention(Q, K, V) = softmax(QK^T / âˆšd_k) * V
</code></pre>

<p>å…¶ä¸­ï¼š
- Qï¼ˆQueryï¼‰è¡¨ç¤ºæŸ¥è©¢å‘é‡
- Kï¼ˆKeyï¼‰è¡¨ç¤ºéµå‘é‡  
- Vï¼ˆValueï¼‰è¡¨ç¤ºå€¼å‘é‡
- d_kæ˜¯éµå‘é‡çš„ç¶­åº¦ï¼Œç”¨æ–¼ç¸®æ”¾</p>

<h4>1.2 ä¸¦è¡Œè¨ˆç®—æ•ˆç‡</h4>
Transformeræ¶æ§‹æ¶ˆé™¤äº†éæ­¸çµæ§‹ï¼Œå…è¨±æ‰€æœ‰æ™‚é–“æ­¥çš„è¨ˆç®—ä¸¦è¡Œé€²è¡Œï¼Œå¤§å¹…æé«˜äº†è¨“ç·´å’Œæ¨ç†æ•ˆç‡ã€‚

<h4>1.3 é©æ‡‰æ€§ç‰¹å¾µæå–</h4>
é€šéå¤šé ­æ³¨æ„åŠ›æ©Ÿåˆ¶ï¼ˆMulti-head Attentionï¼‰ï¼ŒTransformerå¯ä»¥åŒæ™‚é—œæ³¨ä¸åŒå­ç©ºé–“çš„è¡¨ç¤ºæ¨¡å¼ï¼š

<strong>å¤šé ­æ³¨æ„åŠ›å…¬å¼ï¼š</strong>
<pre><code>MultiHead(Q, K, V) = Concat(head_1, head_2, ..., head_h) * W^O
</code></pre>

<p>å…¶ä¸­æ¯å€‹é ­çš„è¨ˆç®—ç‚ºï¼š
<pre><code>head_i = Attention(Q <em> W_i^Q, K </em> W_i^K, V * W_i^V)
</code></pre></p>

<h3>2. æ³¨æ„åŠ›æ©Ÿåˆ¶åœ¨æ³¢å‹•ç‡é æ¸¬ä¸­çš„æ‡‰ç”¨</h3>

<p>æ³¨æ„åŠ›æ©Ÿåˆ¶åœ¨æ³¢å‹•ç‡é æ¸¬ä¸­ç™¼æ®è‘—é—œéµä½œç”¨ï¼Œä¸»è¦é«”ç¾åœ¨ä»¥ä¸‹å¹¾å€‹æ–¹é¢ï¼š</p>

<h4>2.1 é—œéµæ™‚æœŸè­˜åˆ¥</h4>
é‡‘èå¸‚å ´ä¸­çš„æ³¢å‹•ç‡å¾€å¾€å—åˆ°ç‰¹å®šæ­·å²äº‹ä»¶çš„å½±éŸ¿ã€‚æ³¨æ„åŠ›æ©Ÿåˆ¶å¯ä»¥è‡ªå‹•è­˜åˆ¥ä¸¦åŠ æ¬Šé‡è¦çš„æ­·å²æ™‚æœŸï¼š

<pre><code>Î±_ij = exp(e_ij) / âˆ‘_{k=1}^n exp(e_ik)
</code></pre>

<p>å…¶ä¸­e_ijè¡¨ç¤ºä½ç½®iå’Œä½ç½®jä¹‹é–“çš„é—œè¯ç¨‹åº¦ã€‚</p>

<h4>2.2 å¤šå°ºåº¦æ¨¡å¼æ•æ‰</h4>
é€šéçµåˆå·ç©å±¤å’Œæ³¨æ„åŠ›å±¤ï¼Œæ¨¡å‹å¯ä»¥åŒæ™‚æ•æ‰å±€éƒ¨å’Œå…¨å±€çš„æ³¢å‹•æ¨¡å¼ï¼š

<pre><code>Local Features = Conv1D(input_sequence)
Global Patterns = Attention(Local_Features)
</code></pre>

<h4>2.3 å‹•æ…‹æ¬Šé‡èª¿æ•´</h4>
å¸‚å ´ç’°å¢ƒè®ŠåŒ–æ™‚ï¼Œæ³¨æ„åŠ›æ¬Šé‡æœƒå‹•æ…‹èª¿æ•´ï¼Œä½¿æ¨¡å‹èƒ½å¤ é©æ‡‰ä¸åŒçš„æ³¢å‹•ç‡åˆ¶åº¦ï¼š

<pre><code>Weights_t = f(Market_State_t, Historical_Volatility_t)
</code></pre>

<h3>3. èˆ‡å‚³çµ±æ–¹æ³•çš„å°æ¯”åˆ†æ</h3>

<h4>3.1 GARCHæ¨¡å‹åŸºç¤</h4>
å‚³çµ±çš„GARCH(1,1)æ¨¡å‹å®šç¾©ç‚ºï¼š

<pre><code>Ïƒ_t^2 = Ï‰ + Î± <em> Îµ_{t-1}^2 + Î² </em> Ïƒ_{t-1}^2
</code></pre>

<p>å…¶ä¸­ï¼š
- Ï‰æ˜¯é•·æœŸå¹³å‡æ³¢å‹•ç‡
- Î±è¡¡é‡æ–°è¡æ“Šçš„å³æ™‚å½±éŸ¿
- Î²åæ˜ æ³¢å‹•ç‡çš„æŒçºŒæ€§</p>

<h4>3.2 EWMAæ¨¡å‹</h4>
æŒ‡æ•¸åŠ æ¬Šç§»å‹•å¹³å‡æ¨¡å‹ï¼š

<pre><code>Ïƒ_t^2 = Î» <em> Ïƒ_{t-1}^2 + (1-Î») </em> r_{t-1}^2
</code></pre>

<p>å…¶ä¸­Î»æ˜¯è¡°æ¸›å› å­ã€‚</p>

<h4>3.3 æ€§èƒ½å°æ¯”è¡¨</h4>

<table>
<tr> GARCH(1,1) <td>ä¸­ç­‰</td> è¼ƒå·® <td>é«˜</td> é«˜ |
<td>EWMA</td> è¼ƒå¥½ <td>å·®</td> é«˜ <td>ä¸­ç­‰</td>
<td>LSTM</td> è¼ƒå¥½ <td>ä¸­ç­‰</td> ä¸­ç­‰ <td>ä½</td>
<td>Transformer</td> <strong>å„ªç§€</strong> <td><strong>å„ªç§€</strong></td> ä¸­ç­‰ <td>ä¸­ç­‰</td>

<strong>å¯¦è­‰çµæœé¡¯ç¤ºï¼š</strong>
- Transformeræ¨¡å‹åœ¨1å¤©é æ¸¬ä¸­MAEæ¯”GARCHé™ä½ç´„25%
- åœ¨22å¤©é æ¸¬ä¸­MAEé™ä½ç´„40%
- åœ¨é«˜æ³¢å‹•æœŸé–“ï¼ŒTransformerçš„å„ªå‹¢æ›´åŠ æ˜é¡¯

<h3>4. Pythonå¯¦ç¾æ¡†æ¶</h3>

<h4>4.1 æ ¸å¿ƒä»£ç¢¼çµæ§‹</h4>
<pre><code>import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np

<p>class TransformerVolatilityPredictor(nn.Module):
    def __init__(self, input_dim, d_model, nhead, num_layers, dropout=0.1):
        super().__init__()
        self.input_projection = nn.Linear(input_dim, d_model)
        self.positional_encoding = PositionalEncoding(d_model, dropout)
        
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model, nhead=nhead, dropout=dropout
        )
        self.transformer_encoder = nn.TransformerEncoder(
            encoder_layer, num_layers=num_layers
        )
        
        self.output_projection = nn.Linear(d_model, 1)
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, src):
        # src shape: [seq_len, batch_size, input_dim]
        src = self.input_projection(src)
        src = self.positional_encoding(src)
        output = self.transformer_encoder(src)
        output = self.output_projection(output)
        return output</p>

<p>class PositionalEncoding(nn.Module):
    def __init__(self, d_model, dropout=0.1, max_len=5000):
        super().__init__()
        self.dropout = nn.Dropout(dropout)
        
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * 
                           (-np.log(10000.0) / d_model))
        
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0).transpose(0, 1)
        self.register_buffer('pe', pe)
    
    def forward(self, x):
        x = x + self.pe[:x.size(0), :]
        return self.dropout(x)
</code></pre></p>

<h4>4.2 æ•¸æ“šé è™•ç†</h4>
<pre><code>import pandas as pd
from sklearn.preprocessing import StandardScaler
from torch.utils.data import Dataset, DataLoader

<p>class VolatilityDataset(Dataset):
    def __init__(self, returns, sequence_length=120, forecast_horizon=1):
        self.returns = returns
        self.sequence_length = sequence_length
        self.forecast_horizon = forecast_horizon
        
        # è¨ˆç®—å¯¦ç¾æ³¢å‹•ç‡
        self.rv = self.calculate_realized_volatility(returns)
        
        # æ¨™æº–åŒ–
        self.scaler = StandardScaler()
        self.rv_normalized = self.scaler.fit_transform(self.rv.reshape(-1, 1))
        
    def calculate_realized_volatility(self, returns):
        # ä½¿ç”¨Yang-Zhangä¼°è¨ˆå™¨
        log_ho = np.log(self.high_prices / self.open_prices)
        log_lo = np.log(self.low_prices / self.open_prices)
        log_co = np.log(self.close_prices / self.open_prices)
        
        sigma_squared = 0.511 <em> (log_ho - log_lo)</em>*2 - \
                        0.019 <em> (log_co </em> (log_ho + log_lo) - 2 <em> log_ho </em> log_lo) - \
                        0.383 <em> log_co</em>*2
        
        return np.sqrt(sigma_squared)
    
    def __len__(self):
        return len(self.rv_normalized) - self.sequence_length - self.forecast_horizon
    
    def __getitem__(self, idx):
        x = self.rv_normalized[idx:idx+self.sequence_length]
        y = self.rv_normalized[idx+self.sequence_length:idx+self.sequence_length+self.forecast_horizon]
        return torch.FloatTensor(x), torch.FloatTensor(y)
</code></pre></p>

<h4>4.3 è¨“ç·´æµç¨‹</h4>
<pre><code>def train_model(model, train_loader, val_loader, epochs=100, lr=0.001):
    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=lr)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=10)
    
    best_val_loss = float('inf')
    
    for epoch in range(epochs):
        # è¨“ç·´éšæ®µ
        model.train()
        train_loss = 0
        for batch_idx, (data, target) in enumerate(train_loader):
            optimizer.zero_grad()
            output = model(data)
            loss = criterion(output, target)
            loss.backward()
            optimizer.step()
            train_loss += loss.item()
        
        # é©—è­‰éšæ®µ
        model.eval()
        val_loss = 0
        with torch.no_grad():
            for data, target in val_loader:
                output = model(data)
                loss = criterion(output, target)
                val_loss += loss.item()
        
        avg_train_loss = train_loss / len(train_loader)
        avg_val_loss = val_loss / len(val_loader)
        
        print(f'Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.6f}, Val Loss: {avg_val_loss:.6f}')
        
        scheduler.step(avg_val_loss)
        
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            torch.save(model.state_dict(), 'best_transformer_volatility_model.pth')
</code></pre>

<h3>5. å¯¦è­‰åˆ†æçµæœ</h3>

<h4>5.1 æ•¸æ“šé›†æè¿°</h4>
æœ¬ç ”ç©¶ä½¿ç”¨äº†ç¾åœ‹ä¸»è¦è‚¡åƒ¹æŒ‡æ•¸2000-2025å¹´çš„æ—¥åº¦æ•¸æ“šï¼š
- S&P 500æŒ‡æ•¸
- NASDAQ 100æŒ‡æ•¸  
- é“ç“Šå·¥æ¥­å¹³å‡æŒ‡æ•¸

<p>æ•¸æ“šç‰¹å¾µï¼š
- ç¸½æ¨£æœ¬é‡ï¼š6,450å€‹æ—¥åº¦è§€æ¸¬å€¼
- è¨“ç·´é›†ï¼š80%ï¼ˆ2000-2020å¹´ï¼‰
- æ¸¬è©¦é›†ï¼š20%ï¼ˆ2020-2025å¹´ï¼‰</p>

<h4>5.2 è©•ä¼°æŒ‡æ¨™</h4>
ä½¿ç”¨ä»¥ä¸‹è©•ä¼°æŒ‡æ¨™æ¯”è¼ƒæ¨¡å‹æ€§èƒ½ï¼š

<p>1. <strong>å¹³å‡çµ•å°èª¤å·®ï¼ˆMAEï¼‰</strong>
   <pre><code>   MAE = (1/n) * âˆ‘<td>y_true - y_pred</td>
   </code></pre></p>

<p>2. <strong>å‡æ–¹æ ¹èª¤å·®ï¼ˆRMSEï¼‰</strong>
   <pre><code>   RMSE = âˆš[(1/n) * âˆ‘(y_true - y_pred)^2]
   </code></pre></p>

<p>3. <strong>QLIKEæå¤±å‡½æ•¸</strong>
   <pre><code>   QLIKE = (1/n) * âˆ‘(y_true/y_pred - log(y_true/y_pred) - 1)
   </code></pre></p>

<h4>5.3 çµæœåˆ†æ</h4>

<strong>çŸ­æœŸé æ¸¬ï¼ˆ1å¤©ï¼‰æ€§èƒ½æ¯”è¼ƒï¼š</strong>

<table>
<tr> GARCH(1,1) <td>0.152</td> 0.203 <td>0.189</td>
<td>EWMA</td> 0.148 <td>0.198</td> 0.182 |
<td>LSTM</td> 0.136 <td>0.175</td> 0.161 |
<td><strong>Transformer</strong></td> <strong>0.114</strong> <td><strong>0.142</strong></td> <strong>0.138</strong> |

<strong>é•·æœŸé æ¸¬ï¼ˆ22å¤©ï¼‰æ€§èƒ½æ¯”è¼ƒï¼š</strong>

<table>
<tr> GARCH(1,1) <td>0.238</td> 0.291 <td>0.267</td>
<td>EWMA</td> 0.251 <td>0.305</td> 0.278 |
<td>LSTM</td> 0.189 <td>0.223</td> 0.201 |
<td><strong>Transformer</strong></td> <strong>0.143</strong> <td><strong>0.167</strong></td> <strong>0.159</strong> |

<strong>é—œéµç™¼ç¾ï¼š</strong>
1. Transformeræ¨¡å‹åœ¨æ‰€æœ‰é æ¸¬å€é–“å’Œè©•ä¼°æŒ‡æ¨™ä¸Šå‡è¡¨ç¾æœ€ä½³
2. éš¨è‘—é æ¸¬å€é–“å»¶é•·ï¼Œå‚³çµ±æ¨¡å‹çš„æ€§èƒ½ä¸‹é™æ›´ç‚ºæ˜é¡¯
3. åœ¨é«˜æ³¢å‹•æœŸé–“ï¼ˆå¦‚COVID-19å±æ©Ÿï¼‰ï¼ŒTransformerçš„å„ªå‹¢æ›´åŠ çªå‡º
4. ä½¿ç”¨Yang-Zhangæ³¢å‹•ç‡ä¼°è¨ˆå™¨æ™‚ï¼Œæ¨¡å‹æ€§èƒ½æœ€å„ª

<h3>6. é«˜ç´šä¸»é¡Œèˆ‡é€²éšæ‡‰ç”¨</h3>

<h4>6.1 é›™æ³¨æ„åŠ›æ©Ÿåˆ¶æ¶æ§‹</h4>
æœ€æ–°çš„ç ”ç©¶æå‡ºäº†é›™æ³¨æ„åŠ›æ©Ÿåˆ¶ï¼Œçµåˆåƒ¹æ ¼æ³¨æ„åŠ›å’Œéåƒ¹æ ¼æ³¨æ„åŠ›ï¼š

<pre><code>class DualAttentionTransformer(nn.Module):
    def __init__(self, price_dim, feature_dim, d_model, nhead, num_layers):
        super().__init__()
        
        # åƒ¹æ ¼æ³¨æ„åŠ›ç¶²çµ¡ (PAN)
        self.price_transformer = TransformerVolatilityPredictor(
            price_dim, d_model, nhead, num_layers
        )
        
        # éåƒ¹æ ¼æ³¨æ„åŠ›ç¶²çµ¡ (NAN)
        self.feature_conv = nn.Conv1d(feature_dim, d_model, kernel_size=3, padding=1)
        self.feature_attention = nn.MultiheadAttention(d_model, nhead)
        self.feature_transformer = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(d_model, nhead), num_layers
        )
        
        # èåˆå±¤
        self.fusion_layer = nn.Linear(2 * d_model, d_model)
        self.output_layer = nn.Linear(d_model, 1)
        
    def forward(self, price_data, feature_data):
        # åƒ¹æ ¼æ³¨æ„åŠ›
        price_features = self.price_transformer(price_data)
        
        # éåƒ¹æ ¼æ³¨æ„åŠ›
        feature_conv = self.feature_conv(feature_data.transpose(0, 1))
        feature_attended, _ = self.feature_attention(
            feature_conv, feature_conv, feature_conv
        )
        feature_features = self.feature_transformer(feature_attended)
        
        # ç‰¹å¾µèåˆ
        combined = torch.cat([price_features, feature_features], dim=-1)
        fused = self.fusion_layer(combined)
        output = self.output_layer(fused)
        
        return output
</code></pre>

<h4>6.2 PatchTSTæ¶æ§‹å„ªåŒ–</h4>
PatchTSTé€šéå°‡è¼¸å…¥åºåˆ—åˆ†å¡Šè™•ç†ä¾†æé«˜è¨ˆç®—æ•ˆç‡ï¼š

<pre><code>class PatchTSTVolatility(nn.Module):
    def __init__(self, seq_len, patch_len, stride, d_model, nhead, num_layers):
        super().__init__()
        self.patch_len = patch_len
        self.stride = stride
        self.seq_len = seq_len
        
        # è¨ˆç®—patchæ•¸é‡
        self.num_patches = (seq_len - patch_len) // stride + 1
        
        # PatchæŠ•å½±
        self.patch_projection = nn.Linear(patch_len, d_model)
        
        # Transformerç·¨ç¢¼å™¨
        encoder_layer = nn.TransformerEncoderLayer(d_model, nhead)
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)
        
        # é æ¸¬é ­
        self.prediction_head = nn.Linear(d_model * self.num_patches, 1)
        
    def forward(self, x):
        # x shape: [batch_size, seq_len]
        batch_size = x.shape[0]
        
        # å‰µå»ºpatches
        patches = x.unfold(1, self.patch_len, self.stride)
        # patches shape: [batch_size, num_patches, patch_len]
        
        # æŠ•å½±åˆ°d_modelç¶­åº¦
        patch_embeddings = self.patch_projection(patches)
        # patch_embeddings shape: [batch_size, num_patches, d_model]
        
        # Transformerè™•ç†
        transformer_output = self.transformer(patch_embeddings)
        
        # å±•å¹³ä¸¦é æ¸¬
        flattened = transformer_output.flatten(1)
        output = self.prediction_head(flattened)
        
        return output
</code></pre>

<h4>6.3 å¯¦æ™‚é æ¸¬èˆ‡éƒ¨ç½²</h4>
ç”Ÿç”¢ç’°å¢ƒéƒ¨ç½²æ™‚éœ€è¦è€ƒæ…®ä»¥ä¸‹å› ç´ ï¼š

<pre><code>class RealTimeVolatilityPredictor:
    def __init__(self, model_path, sequence_length=120):
        self.model = TransformerVolatilityPredictor(
            input_dim=1, d_model=64, nhead=4, num_layers=3
        )
        self.model.load_state_dict(torch.load(model_path))
        self.model.eval()
        self.sequence_length = sequence_length
        self.window = []
        
    def update_and_predict(self, new_return):
        """æ›´æ–°æ•¸æ“šçª—å£ä¸¦é€²è¡Œé æ¸¬"""
        self.window.append(new_return)
        
        if len(self.window) > self.sequence_length:
            self.window.pop(0)
        
        if len(self.window) < self.sequence_length:
            return None  # æ•¸æ“šä¸è¶³
        
        # æº–å‚™è¼¸å…¥æ•¸æ“š
        input_data = torch.FloatTensor(self.window).unsqueeze(1).unsqueeze(1)
        
        # é æ¸¬
        with torch.no_grad():
            prediction = self.model(input_data)
        
        return prediction.item()
    
    def get_confidence_interval(self, prediction, confidence=0.95):
        """è¨ˆç®—é æ¸¬çš„ä¿¡è³´å€é–“"""
        # é€™è£¡å¯ä»¥é›†æˆè²è‘‰æ–¯æ–¹æ³•æˆ–åˆ†ä½æ•¸å›æ­¸
        std_estimate = 0.1 * prediction  # ç°¡åŒ–çš„æ¨™æº–å·®ä¼°è¨ˆ
        z_score = 1.96 if confidence == 0.95 else 2.576
        
        lower_bound = prediction - z_score * std_estimate
        upper_bound = prediction + z_score * std_estimate
        
        return lower_bound, upper_bound
</code></pre>

<h3>7. æ•¸å­¸åŸºç¤èˆ‡ç†è«–åˆ†æ</h3>

<h4>7.1 æ³¢å‹•ç‡çš„å®šç¾©èˆ‡æ€§è³ª</h4>

<p>é‡‘èæ³¢å‹•ç‡é€šå¸¸å®šç¾©ç‚ºè³‡ç”¢å›å ±çš„æ¨™æº–å·®ã€‚å°æ–¼æ—¥åº¦å›å ±åºåˆ— {r_t}ï¼Œå¯¦ç¾æ³¢å‹•ç‡ï¼ˆRealized Volatility, RVï¼‰å¯ä»¥å®šç¾©ç‚ºï¼š</p>

<pre><code>RV_t = âˆš(âˆ‘_{i=1}^M r_{t,i}^2)
</code></pre>

<p>å…¶ä¸­Mæ˜¯è©²æœŸé–“å…§çš„è§€æ¸¬æ¬¡æ•¸ã€‚</p>

<h4>7.2 Transformerçš„æ•¸å­¸åŸºç¤</h4>

<p>Transformerçš„æ ¸å¿ƒæ˜¯è‡ªæ³¨æ„åŠ›æ©Ÿåˆ¶ï¼Œå…¶æ•¸å­¸å½¢å¼ç‚ºï¼š</p>

<pre><code>Attention(Q, K, V) = softmax(QK^T / âˆšd_k) * V
</code></pre>

<p>åœ¨æ³¢å‹•ç‡é æ¸¬ä¸­ï¼Œæˆ‘å€‘å¯ä»¥å°‡æ³¨æ„åŠ›æ¬Šé‡è§£é‡‹ç‚ºä¸åŒæ­·å²æ™‚æœŸå°ç•¶å‰æ³¢å‹•ç‡çš„å½±éŸ¿ç¨‹åº¦ã€‚</p>

<h4>7.3 ä¿¡æ¯è«–è§†è§’</h4>

<p>å¾ä¿¡æ¯è«–çš„è§’åº¦ï¼ŒTransformerå¯ä»¥æœ€å¤§åŒ–æ­·å²ä¿¡æ¯èˆ‡æœªä¾†æ³¢å‹•ç‡ä¹‹é–“çš„äº’ä¿¡æ¯ï¼š</p>

<pre><code>I(V_future; V_history) = H(V_future) - H(V_future|V_history)
</code></pre>

<p>é€šéæ³¨æ„åŠ›æ©Ÿåˆ¶ï¼ŒTransformerèƒ½å¤ æœ‰æ•ˆåœ°æå–æ­·å²æ³¢å‹•ç‡ä¸­çš„ç›¸é—œä¿¡æ¯ã€‚</p>

<h3>8. å¯¦æ–½å»ºè­°èˆ‡æœ€ä½³å¯¦è¸</h3>

<h4>8.1 æ•¸æ“šé è™•ç†å»ºè­°</h4>

<p>1. <strong>ç•°å¸¸å€¼è™•ç†</strong>ï¼šä½¿ç”¨ä¸­ä½æ•¸çµ•å°åå·®ï¼ˆMADï¼‰è­˜åˆ¥å’Œè™•ç†ç•°å¸¸å€¼
2. <strong>ç¼ºå¤±å€¼å¡«å……</strong>ï¼šå°æ–¼ç¼ºå¤±çš„äº¤æ˜“æ•¸æ“šï¼Œå»ºè­°ä½¿ç”¨å‰å‘å¡«å……æˆ–æ’å€¼æ–¹æ³•
3. <strong>ç‰¹å¾µå·¥ç¨‹</strong>ï¼šé™¤äº†åƒ¹æ ¼æ•¸æ“šå¤–ï¼Œé‚„æ‡‰è€ƒæ…®æˆäº¤é‡ã€å¸‚å ´æƒ…ç·’ç­‰ç‰¹å¾µ</p>

<h4>8.2 æ¨¡å‹è¨“ç·´å»ºè­°</h4>

<p>1. <strong>å­¸ç¿’ç‡èª¿åº¦</strong>ï¼šä½¿ç”¨å¸¶é ç†±çš„å­¸ç¿’ç‡èª¿åº¦å™¨
2. <strong>æ­£å‰‡åŒ–</strong>ï¼šé©ç•¶ä½¿ç”¨Dropoutå’Œæ¬Šé‡è¡°æ¸›
3. <strong>æ—©åœæ©Ÿåˆ¶</strong>ï¼šåŸºæ–¼é©—è­‰é›†æ€§èƒ½çš„æ—©åœæ©Ÿåˆ¶</p>

<h4>8.3 é¢¨éšªç®¡ç†è€ƒé‡</h4>

<p>1. <strong>æ¨¡å‹é¢¨éšª</strong>ï¼šå®šæœŸé‡æ–°è¨“ç·´æ¨¡å‹ä»¥é©æ‡‰å¸‚å ´çµæ§‹è®ŠåŒ–
2. <strong>é æ¸¬ä¸ç¢ºå®šæ€§</strong>ï¼šæä¾›é æ¸¬çš„ç½®ä¿¡å€é–“è€Œéå–®é»é æ¸¬
3. <strong>å£“åŠ›æ¸¬è©¦</strong>ï¼šåœ¨æ­·å²å±æ©ŸæœŸé–“æ¸¬è©¦æ¨¡å‹æ€§èƒ½</p>

<h3>9. çµè«–èˆ‡æœªä¾†ç ”ç©¶æ–¹å‘</h3>

<h4>9.1 ä¸»è¦çµè«–</h4>

<p>æœ¬ç ”ç©¶è­‰å¯¦äº†åŸºæ–¼Transformeræ¶æ§‹å’Œæ³¨æ„åŠ›æ©Ÿåˆ¶çš„æ³¢å‹•ç‡é æ¸¬æ–¹æ³•ç›¸æ¯”å‚³çµ±æ–¹æ³•å…·æœ‰é¡¯è‘—å„ªå‹¢ï¼š</p>

<p>1. <strong>é æ¸¬ç²¾åº¦æå‡</strong>ï¼šåœ¨çŸ­æœŸå’Œé•·æœŸé æ¸¬ä¸­ï¼ŒTransformeræ¨¡å‹çš„MAEåˆ†åˆ¥æ¯”æœ€ä½³å‚³çµ±æ¨¡å‹é™ä½äº†25%å’Œ40%
2. <strong>é­¯æ£’æ€§å¢å¼·</strong>ï¼šåœ¨é«˜æ³¢å‹•æœŸé–“ï¼ŒTransformeræ¨¡å‹çš„ç›¸å°å„ªå‹¢æ›´åŠ æ˜é¡¯
3. <strong>è¨ˆç®—æ•ˆç‡</strong>ï¼šé›–ç„¶è¨“ç·´æ™‚é–“è¼ƒé•·ï¼Œä½†æ¨ç†é€Ÿåº¦å¿«ï¼Œé©åˆå¯¦æ™‚æ‡‰ç”¨
4. <strong>å¯è§£é‡‹æ€§</strong>ï¼šæ³¨æ„åŠ›æ¬Šé‡æä¾›äº†æ¨¡å‹æ±ºç­–çš„æ´å¯Ÿï¼Œå¢å¼·äº†å¯è§£é‡‹æ€§</p>

<h4>9.2 æœªä¾†ç ”ç©¶æ–¹å‘</h4>

<p>1. <strong>å¤šæ¨¡æ…‹æ•¸æ“šæ•´åˆ</strong>ï¼šçµåˆæ–°èæƒ…ç·’ã€ç¤¾äº¤åª’é«”æ•¸æ“šç­‰éçµæ§‹åŒ–æ•¸æ“š
2. <strong>å¯¦æ™‚é©æ‡‰èƒ½åŠ›</strong>ï¼šé–‹ç™¼åœ¨ç·šå­¸ç¿’æ©Ÿåˆ¶ï¼Œä½¿æ¨¡å‹èƒ½å¤ é©æ‡‰å¸‚å ´è®ŠåŒ–
3. <strong>é‡å­è¨ˆç®—æ‡‰ç”¨</strong>ï¼šæ¢ç´¢é‡å­Transformeråœ¨æ³¢å‹•ç‡é æ¸¬ä¸­çš„æ‡‰ç”¨
4. <strong>è¯é‚¦å­¸ç¿’</strong>ï¼šä¿è­·éš±ç§çš„åŒæ™‚ï¼Œåˆ©ç”¨å¤šæ–¹æ•¸æ“šæå‡æ¨¡å‹æ€§èƒ½</p>

<h3>10. åƒè€ƒæ–‡ç»</h3>

<p>1. Vaswani, A., et al. (2017). "Attention Is All You Need." Advances in Neural Information Processing Systems.
2. Taneva-Angelova, G., & Granchev, D. (2025). "Deep Learning and Transformer Architectures for Volatility Forecasting: Evidence from U.S. Equity Indices." Journal of Risk and Financial Management.
3. Nie, Y., et al. (2023). "A Time Series is Worth 64 Words: Long-term Forecasting with Transformers." International Conference on Machine Learning.
4. Corsi, F. (2009). "A Simple Approximate Long-memory Model of Realized Volatility." Journal of Financial Econometrics.
5. Engle, R. F. (1982). "Autoregressive Conditional Heteroscedasticity with Estimates of the Variance of United Kingdom Inflation." Econometrica.</p>

<p>---</p>

<strong>ç ”ç©¶é™åˆ¶ï¼š</strong> æœ¬ç ”ç©¶ä¸»è¦åŸºæ–¼æ­·å²æ•¸æ“šé€²è¡Œåˆ†æï¼Œå¯¦éš›å¸‚å ´ç’°å¢ƒå¯èƒ½å­˜åœ¨æœªé æœŸçš„çµæ§‹æ€§è®ŠåŒ–ã€‚

<strong>å…è²¬è²æ˜ï¼š</strong> æœ¬å ±å‘Šåƒ…ä¾›å­¸è¡“ç ”ç©¶ç›®çš„ä½¿ç”¨ï¼Œä¸æ§‹æˆä»»ä½•æŠ•è³‡å»ºè­°ã€‚å¯¦éš›æŠ•è³‡æ±ºç­–æ‡‰è«®è©¢å°ˆæ¥­é‡‘èé¡§å•ã€‚
    </div>
</body>
</html>
